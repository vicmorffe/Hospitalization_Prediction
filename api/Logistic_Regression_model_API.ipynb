{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset/H_MHAS_c2.dta\" \n",
    "df = pd.read_stata(data_path)\n",
    "column_names = list(df.columns.str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pre-Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['r1hosp1y',\n",
    " 'r2hosp1y',\n",
    " 'r3hosp1y',\n",
    " 'r4hosp1y',\n",
    " 'r5hosp1y']\n",
    "target_columns_count = len(target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_wave_columns = [\n",
    "\"unhhidnp\",\n",
    "\"ragender\",\n",
    "]\n",
    "no_wave_columns_count = len(no_wave_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "picked_columns = [\n",
    "    # Age at Interview (Months and Years)\n",
    "    \"r1agey\",\n",
    "    \"r2agey\",\n",
    "    \"r3agey\",\n",
    "    \"r4agey\",\n",
    "    \"r5agey\",\n",
    "    # Self-Report of Health\n",
    "    \"r1shlt\",\n",
    "    \"r2shlt\",\n",
    "    \"r3shlt\",\n",
    "    \"r4shlt\",\n",
    "    \"r5shlt\",\n",
    "    # Doctor Diagnosed Health Problems: Ever Have Condition\n",
    "    \"r1hibpe\",\n",
    "    \"r2hibpe\",\n",
    "    \"r3hibpe\",\n",
    "    \"r4hibpe\",\n",
    "    \"r5hibpe\",\n",
    "    \"r1diabe\",\n",
    "    \"r2diabe\",\n",
    "    \"r3diabe\",\n",
    "    \"r4diabe\",\n",
    "    \"r5diabe\",\n",
    "    \"r1cancre\",\n",
    "    \"r2cancre\",\n",
    "    \"r3cancre\",\n",
    "    \"r4cancre\",\n",
    "    \"r5cancre\",\n",
    "    \"r1respe\",\n",
    "    \"r2respe\",\n",
    "    \"r3respe\",\n",
    "    \"r4respe\",\n",
    "    \"r5respe\",\n",
    "    \"r1hrtatte\",\n",
    "    \"r2hrtatte\",\n",
    "    \"r3hrtatte\",\n",
    "    \"r4hrtatte\",\n",
    "    \"r5hrtatte\",\n",
    "    \"r4hearte\",\n",
    "    \"r5hearte\",\n",
    "    \"r1stroke\",\n",
    "    \"r2stroke\",\n",
    "    \"r3stroke\",\n",
    "    \"r4stroke\",\n",
    "    \"r5stroke\",\n",
    "    \"r1arthre\",\n",
    "    \"r2arthre\",\n",
    "    \"r3arthre\",\n",
    "    \"r4arthre\",\n",
    "    \"r5arthre\",\n",
    "    \"s5arthre\",\n",
    "    # RwBMI is the respondent's self-reported body mass index\n",
    "    \"r1bmi\",\n",
    "    \"r2bmi\",\n",
    "    \"r3bmi\",\n",
    "    \"r4bmi\",\n",
    "    \"r5bmi\",\n",
    "    # Health Behaviors: Physical Activity or Exercise\n",
    "    \"r1vigact\",\n",
    "    \"r2vigact\",\n",
    "    \"r3vigact\",\n",
    "    \"r4vigact\",\n",
    "    \"r5vigact\",\n",
    "    # Health Behaviors: Smoking (Cigarettes)\n",
    "    \"r1smokev\",\n",
    "    \"r2smokev\",\n",
    "    \"r3smokev\",\n",
    "    \"r4smokev\",\n",
    "    \"r5smokev\",\n",
    "    \"r1smoken\",\n",
    "    \"r2smoken\",\n",
    "    \"r3smoken\",\n",
    "    \"r4smoken\",\n",
    "    \"r5smoken\",\n",
    "    \"r1smokef\",\n",
    "    \"r2smokef\",\n",
    "    \"r3smokef\",\n",
    "    \"r4smokef\",\n",
    "    \"r5smokef\",\n",
    "    \"r1strtsmok\",\n",
    "    \"r2strtsmok\",\n",
    "    \"r3strtsmok\",\n",
    "    \"r4strtsmok\",\n",
    "    \"r5strtsmok\",\n",
    "    \"r1quitsmok\",\n",
    "    \"r2quitsmok\",\n",
    "    \"r3quitsmok\",\n",
    "    \"r4quitsmok\",\n",
    "    \"r5quitsmok\",\n",
    "    # Health Behaviors: Preventive Care\n",
    "    \"r1cholst\",\n",
    "    \"r2cholst\",\n",
    "    \"r3cholst\",\n",
    "    \"r4cholst\",\n",
    "    \"r5cholst\",\n",
    "    \"r3flusht\",\n",
    "    \"r4flusht\",\n",
    "    \"r5flusht\",\n",
    "    \"r1breast\",\n",
    "    \"r2breast\",\n",
    "    \"r3breast\",\n",
    "    \"r4breast\",\n",
    "    \"r5breast\",\n",
    "    \"r1mammog\",\n",
    "    \"r2mammog\",\n",
    "    \"r3mammog\",\n",
    "    \"r4mammog\",\n",
    "    \"r5mammog\",\n",
    "    \"r1papsm\",\n",
    "    \"r2papsm\",\n",
    "    \"r3papsm\",\n",
    "    \"r4papsm\",\n",
    "    \"r5papsm\",\n",
    "    \"r1prost\",\n",
    "    \"r2prost\",\n",
    "    \"r3prost\",\n",
    "    \"r4prost\",\n",
    "    \"r5prost\",\n",
    "    # ADL Help\n",
    "    \"r1dresshlp\",\n",
    "    \"r2dresshlp\",\n",
    "    \"r3dresshlp\",\n",
    "    \"r4dresshlp\",\n",
    "    \"r5dresshlp\",\n",
    "    \"r1walkhlp\",\n",
    "    \"r2walkhlp\",\n",
    "    \"r3walkhlp\",\n",
    "    \"r4walkhlp\",\n",
    "    \"r5walkhlp\",\n",
    "    \"r1bathehlp\",\n",
    "    \"r2bathehlp\",\n",
    "    \"r3bathehlp\",\n",
    "    \"r4bathehlp\",\n",
    "    \"r5bathehlp\",\n",
    "    \"r1eathlp\",\n",
    "    \"r2eathlp\",\n",
    "    \"r3eathlp\",\n",
    "    \"r4eathlp\",\n",
    "    \"r5eathlp\",\n",
    "    \"r1bedhlp\",\n",
    "    \"r2bedhlp\",\n",
    "    \"r3bedhlp\",\n",
    "    \"r4bedhlp\",\n",
    "    \"r5bedhlp\",\n",
    "    \"r1toilethlp\",\n",
    "    \"r2toilethlp\",\n",
    "    \"r3toilethlp\",\n",
    "    \"r4toilethlp\",\n",
    "    \"r5toilethlp\",\n",
    "    # IADL Help\n",
    "    \"r1mealhlp\",\n",
    "    \"r2mealhlp\",\n",
    "    \"r3mealhlp\",\n",
    "    \"r4mealhlp\",\n",
    "    \"r5mealhlp\",\n",
    "    \"r1shophlp\",\n",
    "    \"r2shophlp\",\n",
    "    \"r3shophlp\",\n",
    "    \"r4shophlp\",\n",
    "    \"r5shophlp\",\n",
    "    \"r1medhlp\",\n",
    "    \"r2medhlp\",\n",
    "    \"r3medhlp\",\n",
    "    \"r4medhlp\",\n",
    "    \"r5medhlp\",\n",
    "    \"r1moneyhlp\",\n",
    "    \"r2moneyhlp\",\n",
    "    \"r3moneyhlp\",\n",
    "    \"r4moneyhlp\",\n",
    "    \"r5moneyhlp\",\n",
    "    # Whether Uses Personal Aids\n",
    "    \"r1walkre\",\n",
    "    \"r2walkre\",\n",
    "    \"r3walkre\",\n",
    "    \"r4walkre\",\n",
    "    \"r5walkre\",\n",
    "    \"r1bede\",\n",
    "    \"r2bede\",\n",
    "    \"r3bede\",\n",
    "    \"r4bede\",\n",
    "    \"r5bede\",\n",
    "    # Activities of Daily Living: Whether Receives Any Care\n",
    "    \"r1racany\",\n",
    "    \"r2racany\",\n",
    "    \"r3racany\",\n",
    "    \"r4racany\",\n",
    "    \"r5racany\",\n",
    "    # Instrumental Activities of Daily Living: Whether Receives Any Care\n",
    "    \"r1ricany\",\n",
    "    \"r2ricany\",\n",
    "    \"r3ricany\",\n",
    "    \"r4ricany\",\n",
    "    \"r5ricany\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected columns are the union of several groups\n",
    "screening = no_wave_columns + target_columns + picked_columns\n",
    "\n",
    "# Work only in a subset of the dataset\n",
    "df_sub = df[screening]\n",
    "\n",
    "# original name for columns with wave reference \n",
    "wave_vars_orig_name = screening[no_wave_columns_count:]\n",
    "\n",
    "# Remove the wave reference characters\n",
    "from collections import OrderedDict\n",
    "wave_vars_clean_name = list(OrderedDict.fromkeys([name[2:] for name in wave_vars_orig_name]))\n",
    "wave_vars_clean_name_count = len(wave_vars_clean_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decouple the wave as a column from the variables\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Initialize an empty DataFrame with the desired final columns\n",
    "# accumulated_df = pd.DataFrame(columns=final_columns)\n",
    "accumulated_df = None\n",
    "\n",
    "# Loop over the wave numbers\n",
    "for wave in range(1, 6):\n",
    "    # Create new column names for the current wave\n",
    "    _names = [f\"r{wave}{var}\" for var in wave_vars_clean_name]\n",
    "    wave_column_names = [name for name in _names if name in df_sub.columns] # Only keep valid column names\n",
    "    # print(wave_column_names)\n",
    "    column_mapping = {name: name[2:] for name in wave_column_names}  \n",
    "    # print(column_mapping)\n",
    "\n",
    "    # Select the necessary columns from the subset DataFrame and rename them\n",
    "    wave_df = df_sub[no_wave_columns + wave_column_names].copy()\n",
    "    wave_df = wave_df.rename(columns=column_mapping)\n",
    "\n",
    "    # Insert the wave number as a new column\n",
    "    wave_df.insert(0, \"wave\", wave)\n",
    "\n",
    "    # Append the current wave's DataFrame to the accumulated DataFrame\n",
    "    if accumulated_df is None:\n",
    "        accumulated_df = wave_df.copy()\n",
    "    else:\n",
    "        accumulated_df = pd.concat([accumulated_df, wave_df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get clean dataset to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76353, 39)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drop_first_columns_count = 2 # wave and unhhidnp columns are not needed\n",
    "df_clean = accumulated_df.iloc[:,drop_first_columns_count:].dropna(subset=[\"hosp1y\"]).copy()\n",
    "print(df_clean.shape)\n",
    "\n",
    "# Swap ragender with hosp1y column to better readability\n",
    "cols = list(df_clean)  # Get a list of column names\n",
    "cols[0], cols[1] = cols[1], cols[0]  # Swap the first two names\n",
    "\n",
    "df_clean = df_clean.reindex(columns=cols)  # Reindex the DataFrame with the new column order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the df_clean dataframe in train, val and test\n",
    "\n",
    "def split_dataset(df, target, random_state=1):\n",
    "    df_full_train, df_test = train_test_split(df, test_size=.2, random_state=random_state)\n",
    "    df_train, df_val = train_test_split(df_full_train, test_size=.2/.8, random_state=random_state)\n",
    "\n",
    "    # Reset index and get y vectors\n",
    "    df_full_train = df_full_train.reset_index(drop=True)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    y_train = df_train[target].values\n",
    "    y_val = df_val[target].values\n",
    "    y_test = df_test[target].values\n",
    "            \n",
    "    del df_train[target] \n",
    "    del df_val[target] \n",
    "    del df_test[target]\n",
    "\n",
    "    return df_full_train, df_train, df_val, df_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45811, 15271, 15271)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_train, df_train, df_val, df_test, y_train, y_val, y_test = split_dataset(df_clean, \"hosp1y\")\n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = list(df_full_train.dtypes[df_full_train.dtypes.ne(\"category\")].index)[1:] # using 1: to skip hosp1y\n",
    "# numerical\n",
    "categorical = [col for col in list(df_full_train.dtypes.index) if col not in numerical][1:] # using 1: to skip hosp1y\n",
    "# categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Transform a dict to the X nparray ready to be used for predictions\n",
    "# it requires a dictionary with the encoder, imputer and scaler objects generated\n",
    "# during training\n",
    "def transform_data(dict_data, transformers):\n",
    "    dv = transformers[\"dv\"]\n",
    "    imputer = transformers[\"imputer\"]\n",
    "    scaler = transformers[\"scaler\"]\n",
    "\n",
    "    X = dv.transform(dict_data)\n",
    "    X = imputer.transform(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# Train the model and fit the encoder, imputer and scaler \n",
    "# returns the model and a dictionary with the encoder, imputer and scaler\n",
    "def train(df, y, C=1.0):\n",
    "    dict_data = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    dv.fit(dict_data)\n",
    "\n",
    "    X = dv.transform(dict_data)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputer.fit(X)\n",
    "\n",
    "    X = imputer.transform(X)\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    scaler.fit(X)\n",
    "\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    transformers = {\"dv\": dv, \"imputer\":imputer, \"scaler\":scaler}\n",
    "\n",
    "    return transformers, model\n",
    "\n",
    "# Predict a full dataframe using the encoder, imputer and scaler and obviously the model\n",
    "def predict(df, transformers, model):\n",
    "    dict_data = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    X = transform_data(dict_data, transformers)\n",
    "\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the previously created functions\n",
    "C = 1.0\n",
    "transformers, model = train(df_train, y_train, C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dv': DictVectorizer(sparse=False), 'imputer': SimpleImputer(strategy='median'), 'scaler': MinMaxScaler()}\n",
      "LogisticRegression(max_iter=1000, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "print(transformers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7348594096930517"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the AUC Score of the validation dataset\n",
    "y_pred = predict(df_val, transformers, model)\n",
    "roc_auc_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare train and eval perfomance\n",
    "\n",
    "# Train data predictions (class 1)\n",
    "log_reg_train = predict(df_train, transformers, model)\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "log_reg_val = predict(df_val, transformers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC Score: 0.7367\n",
      "Validation ROC AUC Score: 0.7349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train = roc_auc_score(y_true=y_train, y_score=log_reg_train)\n",
    "print(f\"Train ROC AUC Score: {roc_auc_train:.4f}\")\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = roc_auc_score(y_true=y_val, y_score=log_reg_val)\n",
    "print(f\"Validation ROC AUC Score: {roc_auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7530830512054058"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the test score\n",
    "y_pred = predict(df_test, transformers, model)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_type = \"logistic_model_test\" # This is the name for the filename of the model\n",
    "\n",
    "output_file = f\"hospitalization-{model_type}.bin\"\n",
    "\n",
    "with open(output_file, \"wb\") as f_out:\n",
    "    pickle.dump((transformers, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dv': DictVectorizer(sparse=False),\n",
       "  'imputer': SimpleImputer(strategy='median'),\n",
       "  'scaler': MinMaxScaler()},\n",
       " LogisticRegression(max_iter=1000, solver='liblinear'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "input_file = f\"hospitalization-{model_type}.bin\" # Use the same name than above\n",
    "\n",
    "with open(input_file, \"rb\") as f_in:\n",
    "    transformers, model = pickle.load(f_in)\n",
    "\n",
    "transformers, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.No\n",
      "{'ragender': '2.Woman', 'agey': 61.0, 'shlt': '2.Very good', 'hibpe': '0.no', 'diabe': '0.no', 'cancre': '0.no', 'respe': '1.yes', 'hrtatte': '0.no', 'stroke': '0.no', 'arthre': '0.no', 'vigact': '1.Yes', 'smokev': '0.No', 'smoken': '0.No', 'cholst': '1.Yes', 'breast': '1.Yes', 'mammog': '0.No', 'papsm': '1.Yes', 'prost': nan, 'dresshlp': nan, 'walkhlp': nan, 'bathehlp': nan, 'eathlp': nan, 'bedhlp': nan, 'toilethlp': nan, 'mealhlp': nan, 'shophlp': nan, 'medhlp': nan, 'moneyhlp': nan, 'walkre': nan, 'bede': nan, 'racany': nan, 'ricany': nan, 'flusht': '1.Yes', 'hearte': nan, 'bmi': nan, 'smokef': 0.0, 'strtsmok': nan, 'quitsmok': nan}\n"
     ]
    }
   ],
   "source": [
    "pos = 157 # change this number to extract a different patient\n",
    "\n",
    "print(y_test[pos]) # Print it the patient was hospitalizated \n",
    "\n",
    "# Convert patient to dict similar to format of the json in the api\n",
    "patient = df_test[categorical + numerical].to_dict(orient='records')[pos]\n",
    "\n",
    "print(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient[\"prost\"] # Beware of nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_patient(patient, transformers):\n",
    "    \"\"\"\n",
    "    Predicts the probability of hospitalization for a single patient.\n",
    "\n",
    "    Parameters:\n",
    "    - patient: The data of the patient to be predicted.\n",
    "    - transformers: The transformers used to preprocess the data.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: The predicted probability of hospitalization.\n",
    "    - hospitalization: True if the predicted probability is greater than or equal to 0.5, False otherwise.\n",
    "    \"\"\"\n",
    "    X_patient = transform_data([patient], transformers)\n",
    "    y_pred = model.predict_proba(X_patient)[0, 1]\n",
    "    hospitalization = y_pred >= 0.5\n",
    "    return y_pred, hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.No', (0.055877408312371896, False))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[pos], predict_single_patient(patient, transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragender': '2.Woman',\n",
       " 'agey': 61.0,\n",
       " 'shlt': '2.Very good',\n",
       " 'hibpe': '0.no',\n",
       " 'diabe': '0.no',\n",
       " 'cancre': '0.no',\n",
       " 'respe': '1.yes',\n",
       " 'hrtatte': '0.no',\n",
       " 'stroke': '0.no',\n",
       " 'arthre': '0.no',\n",
       " 'vigact': '1.Yes',\n",
       " 'smokev': '0.No',\n",
       " 'smoken': '0.No',\n",
       " 'cholst': '1.Yes',\n",
       " 'breast': '1.Yes',\n",
       " 'mammog': '0.No',\n",
       " 'papsm': '1.Yes',\n",
       " 'prost': nan,\n",
       " 'dresshlp': nan,\n",
       " 'walkhlp': nan,\n",
       " 'bathehlp': nan,\n",
       " 'eathlp': nan,\n",
       " 'bedhlp': nan,\n",
       " 'toilethlp': nan,\n",
       " 'mealhlp': nan,\n",
       " 'shophlp': nan,\n",
       " 'medhlp': nan,\n",
       " 'moneyhlp': nan,\n",
       " 'walkre': nan,\n",
       " 'bede': nan,\n",
       " 'racany': nan,\n",
       " 'ricany': nan,\n",
       " 'flusht': '1.Yes',\n",
       " 'hearte': nan,\n",
       " 'bmi': nan,\n",
       " 'smokef': 0.0,\n",
       " 'strtsmok': nan,\n",
       " 'quitsmok': nan}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragender': '2.Woman',\n",
       " 'agey': 61.0,\n",
       " 'shlt': '2.Very good',\n",
       " 'hibpe': '0.no',\n",
       " 'diabe': '0.no',\n",
       " 'cancre': '0.no',\n",
       " 'respe': '1.yes',\n",
       " 'hrtatte': '0.no',\n",
       " 'stroke': '0.no',\n",
       " 'arthre': '0.no',\n",
       " 'vigact': '1.Yes',\n",
       " 'smokev': '0.No',\n",
       " 'smoken': '0.No',\n",
       " 'cholst': '1.Yes',\n",
       " 'breast': '1.Yes',\n",
       " 'mammog': '0.No',\n",
       " 'papsm': '1.Yes',\n",
       " 'prost': '',\n",
       " 'dresshlp': '',\n",
       " 'walkhlp': '',\n",
       " 'bathehlp': '',\n",
       " 'eathlp': '',\n",
       " 'bedhlp': '',\n",
       " 'toilethlp': '',\n",
       " 'mealhlp': '',\n",
       " 'shophlp': '',\n",
       " 'medhlp': '',\n",
       " 'moneyhlp': '',\n",
       " 'walkre': '',\n",
       " 'bede': '',\n",
       " 'racany': '',\n",
       " 'ricany': '',\n",
       " 'flusht': '1.Yes',\n",
       " 'hearte': '',\n",
       " 'bmi': '',\n",
       " 'smokef': 0.0,\n",
       " 'strtsmok': '',\n",
       " 'quitsmok': ''}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to clean the dictionary from NaN in order to dump to a json file\n",
    "patient_clean = dict(patient) # Create a copy of the original patient\n",
    "\n",
    "for key, value in patient_clean.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        patient_clean[key] = value.tolist()  # Convert ndarray to list for JSON compatibility\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        patient_clean[key] = \"\"  # Replace np.nan with string \"\"\n",
    "\n",
    "patient_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_filename = \"lr-single_patient\"\n",
    "with open(f\"{json_filename}.json\", \"w\") as fp:\n",
    "    json.dump(patient_clean, fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will be used in the API codebase, it is tested here for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragender': '2.Woman', 'agey': 61.0, 'shlt': '2.Very good', 'hibpe': '0.no', 'diabe': '0.no', 'cancre': '0.no', 'respe': '1.yes', 'hrtatte': '0.no', 'stroke': '0.no', 'arthre': '0.no', 'vigact': '1.Yes', 'smokev': '0.No', 'smoken': '0.No', 'cholst': '1.Yes', 'breast': '1.Yes', 'mammog': '0.No', 'papsm': '1.Yes', 'prost': '', 'dresshlp': '', 'walkhlp': '', 'bathehlp': '', 'eathlp': '', 'bedhlp': '', 'toilethlp': '', 'mealhlp': '', 'shophlp': '', 'medhlp': '', 'moneyhlp': '', 'walkre': '', 'bede': '', 'racany': '', 'ricany': '', 'flusht': '1.Yes', 'hearte': '', 'bmi': '', 'smokef': 0.0, 'strtsmok': '', 'quitsmok': ''}\n"
     ]
    }
   ],
   "source": [
    "input_file = \"lr-single_patient.json\"\n",
    "\n",
    "with open(input_file, \"rb\") as f_in:\n",
    "    patient_json = json.load(f_in)\n",
    "print(patient_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revive_nan(data):\n",
    "    \"\"\"\n",
    "    Recursively replaces empty strings with NaN values in a nested dictionary or list.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict or list): The input data to be processed.\n",
    "\n",
    "    Returns:\n",
    "    dict or list: The processed data with empty strings replaced by NaN values.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            data[key] = revive_nan(value)\n",
    "    elif isinstance(data, list):\n",
    "        for i, value in enumerate(data):\n",
    "            data[i] = revive_nan(value)\n",
    "    elif data == \"\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "revive_nan(patient_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ragender': '2.Woman',\n",
       " 'agey': 61.0,\n",
       " 'shlt': '2.Very good',\n",
       " 'hibpe': '0.no',\n",
       " 'diabe': '0.no',\n",
       " 'cancre': '0.no',\n",
       " 'respe': '1.yes',\n",
       " 'hrtatte': '0.no',\n",
       " 'stroke': '0.no',\n",
       " 'arthre': '0.no',\n",
       " 'vigact': '1.Yes',\n",
       " 'smokev': '0.No',\n",
       " 'smoken': '0.No',\n",
       " 'cholst': '1.Yes',\n",
       " 'breast': '1.Yes',\n",
       " 'mammog': '0.No',\n",
       " 'papsm': '1.Yes',\n",
       " 'prost': nan,\n",
       " 'dresshlp': nan,\n",
       " 'walkhlp': nan,\n",
       " 'bathehlp': nan,\n",
       " 'eathlp': nan,\n",
       " 'bedhlp': nan,\n",
       " 'toilethlp': nan,\n",
       " 'mealhlp': nan,\n",
       " 'shophlp': nan,\n",
       " 'medhlp': nan,\n",
       " 'moneyhlp': nan,\n",
       " 'walkre': nan,\n",
       " 'bede': nan,\n",
       " 'racany': nan,\n",
       " 'ricany': nan,\n",
       " 'flusht': '1.Yes',\n",
       " 'hearte': nan,\n",
       " 'bmi': nan,\n",
       " 'smokef': 0.0,\n",
       " 'strtsmok': nan,\n",
       " 'quitsmok': nan}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.055877408312371896, False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single_patient(patient_json, transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API drawboard ends here, below there is code used for other models but not used in the final API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mapper = {\"numerical\": numerical, \"categorical\": categorical}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ragender',\n",
       "  'shlt',\n",
       "  'hibpe',\n",
       "  'diabe',\n",
       "  'cancre',\n",
       "  'respe',\n",
       "  'hrtatte',\n",
       "  'stroke',\n",
       "  'arthre',\n",
       "  'vigact',\n",
       "  'smokev',\n",
       "  'smoken',\n",
       "  'cholst',\n",
       "  'breast',\n",
       "  'mammog',\n",
       "  'papsm',\n",
       "  'prost',\n",
       "  'dresshlp',\n",
       "  'walkhlp',\n",
       "  'bathehlp',\n",
       "  'eathlp',\n",
       "  'bedhlp',\n",
       "  'toilethlp',\n",
       "  'mealhlp',\n",
       "  'shophlp',\n",
       "  'medhlp',\n",
       "  'moneyhlp',\n",
       "  'walkre',\n",
       "  'bede',\n",
       "  'racany',\n",
       "  'ricany',\n",
       "  'flusht',\n",
       "  'hearte'],\n",
       " ['agey', 'bmi', 'smokef', 'strtsmok', 'quitsmok'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc_mapper[\"categorical\"], enc_mapper[\"numerical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "def preprocess_data(df_train, df_val, df_test, enc_mapper):\n",
    "    # Print shape of input data\n",
    "    print(\"Input train data shape: \", df_train.shape)\n",
    "    print(\"Input val data shape: \", df_val.shape)\n",
    "    print(\"Input test data shape: \", df_test.shape, \"\\n\")\n",
    "\n",
    "    # Make a copy of the dataframes\n",
    "    working_train_df = df_train.copy()\n",
    "    working_val_df = df_val.copy()\n",
    "    working_test_df = df_test.copy()\n",
    "\n",
    "    # Group cols by type\n",
    "    categorical = enc_mapper[\"categorical\"]\n",
    "    numerical = enc_mapper[\"numerical\"]\n",
    "\n",
    "    X_train = working_train_df[numerical].values\n",
    "    X_val = working_val_df[numerical].values\n",
    "    X_test = working_test_df[numerical].values\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    # Fit on the training data\n",
    "    ohe.fit(working_train_df[categorical])\n",
    "\n",
    "    # Transform train, val and test data\n",
    "    X_train = np.concatenate((X_train, ohe.transform(working_train_df[categorical]).todense()), axis=1)\n",
    "    X_val = np.concatenate((X_val, ohe.transform(working_val_df[categorical]).todense()), axis=1)\n",
    "    X_test = np.concatenate((X_test, ohe.transform(working_test_df[categorical]).todense()), axis=1)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_val = np.asarray(X_val)\n",
    "    X_test = np.asarray(X_test)\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputer.fit(X_train)\n",
    "\n",
    "    X_train = imputer.transform(X_train)\n",
    "    X_val = imputer.transform(X_val)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input train data shape:  (45811, 38)\n",
      "Input val data shape:  (15271, 38)\n",
      "Input test data shape:  (15271, 38) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, X_val, X_test = preprocess_data(df_train, df_val, df_test, enc_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 278 ms, sys: 5.57 ms, total: 283 ms\n",
      "Wall time: 277 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.0001, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.0001, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.0001, max_iter=1000, solver='liblinear')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "log_reg = None\n",
    "log_reg = LogisticRegression(C=0.0001, solver='liblinear', max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data predictions (class 1)\n",
    "log_reg_train = log_reg.predict_proba(X)[:, 1]\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "log_reg_val = log_reg.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC Score: 0.7145\n",
      "Validation ROC AUC Score: 0.7185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train = roc_auc_score(y_true=y_train, y_score=log_reg_train)\n",
    "print(f\"Train ROC AUC Score: {roc_auc_train:.4f}\")\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = roc_auc_score(y_true=y_val, y_score=log_reg_val)\n",
    "print(f\"Validation ROC AUC Score: {roc_auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 41.8 ms, total: 11 s\n",
      "Wall time: 1.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=47, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=47, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=47, verbose=1)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 47, verbose = 1, n_jobs = -1, max_depth=10)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Train data predictions (class 1)\n",
    "rf_pred_train = rf.predict_proba(X)[:, 1]\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "rf_pred_val = rf.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC Score: 0.7832\n",
      "Validation ROC AUC Score: 0.7295\n"
     ]
    }
   ],
   "source": [
    "roc_auc_train = roc_auc_score(y_true=y_train, y_score=rf_pred_train)\n",
    "print(f\"Train ROC AUC Score: {roc_auc_train:.4f}\")\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = roc_auc_score(y_true=y_val, y_score=rf_pred_val)\n",
    "print(f\"Validation ROC AUC Score: {roc_auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.14511471e-02, 9.60600421e-02, 9.59689377e-03, 3.99633498e-02,\n",
       "       3.88595647e-02, 3.48119807e-02, 5.12213797e-03, 5.23831865e-03,\n",
       "       1.74946475e-03, 2.56871035e-03, 8.51070857e-03, 1.09811233e-02,\n",
       "       9.21210717e-03, 5.46707466e-04, 9.91863208e-03, 1.00470574e-02,\n",
       "       3.25541079e-04, 9.92392229e-03, 1.05204886e-02, 2.29541418e-04,\n",
       "       5.20682682e-03, 5.70797702e-03, 3.00202350e-04, 7.53026442e-03,\n",
       "       7.54358519e-03, 2.15189107e-04, 6.66865070e-03, 7.42405496e-03,\n",
       "       3.56216576e-04, 4.98124745e-03, 5.16866042e-03, 2.46982411e-04,\n",
       "       1.15919927e-02, 1.18063507e-02, 3.05330170e-04, 1.19825209e-02,\n",
       "       1.15073066e-02, 9.57661409e-04, 1.04077003e-02, 1.06442986e-02,\n",
       "       3.95505510e-05, 5.40721416e-03, 5.31058827e-03, 1.33377501e-04,\n",
       "       9.59803274e-03, 8.12920967e-03, 1.43743565e-03, 8.93001628e-03,\n",
       "       9.17971449e-03, 3.44561554e-03, 8.65518822e-03, 8.72932912e-03,\n",
       "       3.42634580e-03, 7.47488307e-03, 8.66264127e-03, 4.05431506e-03,\n",
       "       5.75623163e-03, 6.06806733e-03, 4.18466493e-03, 4.79868396e-03,\n",
       "       3.68581109e-03, 5.89149800e-03, 3.19000577e-03, 6.41410058e-03,\n",
       "       8.16199380e-03, 1.69333666e-03, 5.95519233e-03, 5.10987538e-03,\n",
       "       1.36374657e-03, 2.90937186e-03, 3.64747010e-03, 3.92327814e-03,\n",
       "       4.92494807e-03, 5.67612049e-03, 2.98607676e-03, 3.41624971e-03,\n",
       "       5.41885996e-03, 2.45026327e-03, 4.14556542e-03, 4.80783232e-03,\n",
       "       1.78581679e-03, 5.00767859e-03, 5.92198022e-03, 1.03963085e-03,\n",
       "       2.24601035e-03, 2.50154099e-03, 7.08360472e-04, 1.89233374e-03,\n",
       "       2.23624923e-03, 5.76276644e-03, 5.97311831e-03, 6.14421982e-03,\n",
       "       5.04015336e-03, 3.64770135e-03, 5.04705382e-03, 7.55455317e-03,\n",
       "       7.17656513e-03, 7.16692937e-04, 8.67389375e-03, 8.32134991e-03,\n",
       "       4.90684298e-04, 9.50185555e-03, 9.28901673e-03, 4.53870831e-04,\n",
       "       7.57031782e-03, 7.74931530e-03, 7.69389198e-04, 8.44889747e-03,\n",
       "       8.47388178e-03, 5.92884375e-04, 8.45687519e-03, 8.40979138e-03,\n",
       "       1.20440213e-03, 8.11171122e-03, 7.76031399e-03, 6.50526347e-04,\n",
       "       9.49589596e-03, 9.50399247e-03, 5.39241390e-04, 1.11985452e-02,\n",
       "       1.11168512e-02, 1.10038868e-03, 1.19384504e-02, 1.35144502e-02,\n",
       "       1.09819295e-02, 1.13546590e-02, 7.44427256e-03, 1.10967607e-02])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 763 ms, total: 12.8 s\n",
      "Wall time: 40.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=RandomForestClassifier(n_jobs=-1, random_state=47),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [10, 30, 50],\n",
       "                                        &#x27;n_estimators&#x27;: [10, 20, 50, 100]},\n",
       "                   random_state=47, scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(estimator=RandomForestClassifier(n_jobs=-1, random_state=47),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [10, 30, 50],\n",
       "                                        &#x27;n_estimators&#x27;: [10, 20, 50, 100]},\n",
       "                   random_state=47, scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1, random_state=47)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1, random_state=47)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(n_jobs=-1, random_state=47),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [10, 30, 50],\n",
       "                                        'n_estimators': [10, 20, 50, 100]},\n",
       "                   random_state=47, scoring='roc_auc')"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 47, n_jobs = -1)\n",
    "distributions = {\n",
    "    \"n_estimators\": [10, 20, 50, 100],\n",
    "    'max_depth': [10, 30, 50],\n",
    "}\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=distributions, \n",
    "    scoring='roc_auc',  \n",
    "    n_jobs=-1,\n",
    "    random_state=47)\n",
    "rf_random.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 100, 'max_depth': 10}\n",
      "Best score: 0.7299960438741016\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best params: {rf_random.best_params_}\")\n",
    "print(f\"Best score: {rf_random.best_score_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ne1ai-final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
